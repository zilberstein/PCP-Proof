\documentclass{beamer}

% \usepackage{beamerthemesplit} // Activate for custom appearance
\usepackage{mathtools}

\title{Dinur's Proof of the PCP Theorem}
\author{Tiernan Garsys, Lucas Pe\~{n}a, and Noam Zilberstein}
\date{\today}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Problem Statement}
\subsection{The PCP Theorem}
\subsection{}
\section{Main Result}
\subsection{Previous Approaches}
\subsection{Dinur's Approach}
\section{Technical Overview of the Proof}
\subsection{Definitions}
\subsection{The PCP Theorem}
\section{Proof of Lemmas}
\frame
{
  \frametitle{The PCP Theorem}

  \begin{itemize}
  \item<1-> The class NP is equivalent to the set of languages of problems that can be decided by a probabilistically checkable proof using $O(\log n)$ random bits and $O(1)$ query bits.
  \item<2-> Let PCP$_{\epsilon}[r, q, a]$ be the class of languages with a PCP using $r$ random bits and $q$ queries that each return $a$-bit responses.  If $x\in L$, then the verifier will always accept, if $x\not\in L$ then the verifier will accept with probability at most $\epsilon$.   
  \item<3-> Dinur presents a new proof that NP = PCP$_\frac12[c\log n, q, 1]$ where $c$ and $q$ are constants. 
  \end{itemize}
}

\frame{
  \frametitle{Constraint Graph Definition}
  \begin{itemize}
  \item<1->Let $G=\langle\langle V, E\rangle, \Sigma, \mathcal{C}\rangle$ be a constraint graph (CG) where:
  \begin{itemize}
    \item<2->$\langle V, E\rangle$ is a directed graph
    \item<3-> $\Sigma$ is a constant size set of colors
    \item<4-> $\mathcal C =\{c_e : \Sigma^2\mapsto \{0,1\}\mid e\in E\}$ is a set of constraints 
  \end{itemize}
  \item<5-> $G$ is a YES instance of CG iff:
  \begin{itemize}
    \item<6-> $\exists \sigma: V\mapsto \Sigma$ such that $\forall (u,v)\in E,\; c_{(u,v)}(\sigma(u),\sigma(v)) = 1$
  \end{itemize}
  \item<7-> \textbf{Example:} 3-Coloring:
  \begin{itemize}
    \item<8-> $\Sigma = \{$RED, GREEN, BLUE$\}$
    \item<9-> $\forall (u,v)\in E,$
    $$c_{(u,v)}(\sigma(u), \sigma(v)) = \begin{cases}
      1, & \sigma(u) \neq \sigma(v) \\
      0, & \text{otherwise}
      \end{cases}$$
  \end{itemize}
  \end{itemize}
}
\frame{
  \frametitle{Hardness of Constraint Graphs}
  \begin{itemize}
    \item<1-> Determining if a constraint graph is satisfiable is NP-complete
    \begin{itemize}
      \item<2-> Reduction from $k$-Coloring
      \item<3-> CG $\in$ NP. Coloring is the proof, verification complexity is linear in number of edges
    \end{itemize}
  \end{itemize}
}  

\frame{
\frametitle{Technical Overview}
\begin{itemize}
\item<1->Show that the following are equivalent:
\begin{itemize}
\item<2-> Property 1: NP $\subseteq$ PCP$_{1-\epsilon}[O(\log n), 2, 4]$
\item<3-> Property 2: For any language $L\in$ NP there is a polynomial time transformation $T$ from instances of $L$ to a constraint graphs on 16 colors $\mathcal G_{16}$ such that if $x\in L$ then $\mathcal G_{16}$ is satisfiable and if $x\notin L$ then at most $(1-\epsilon)|E|$ of the constraints are simultaneously satisfiable
\end{itemize}
\item<4->It will suffice to prove Property 2
\item<5->Boost the accuracy of the solution to show that NP$\subseteq$ PCP$_{(1-\epsilon)^c}[O(\log n), 8c, 1]$
\end{itemize}
}

\frame{
\frametitle{Regarding Expanders}
\begin{itemize}
\item<1-> Edge expansion property for a graph $G =\langle V, E\rangle$: $\varphi(G) =\min_{|S|\le\frac n2}\left\{\frac{E(S,\bar s)}{|S|}\right\}$ such that $S\subseteq V$.
\end{itemize}
}

\frame{
\frametitle{The Gap Amplification Lemma}
\begin{block}{Lemma: Gap Amplification}
There exists a constant $0<\alpha<1$, an alphabet $\Sigma$, and a polynomial time reduction mapping the CG $G=\langle\langle V, E\rangle, \Sigma, \mathcal{C}\rangle$ to $G=\langle\langle V', E'\rangle, \Sigma', \mathcal{C}'\rangle$ such that:
\begin{itemize}
\item<2->$|G'|$ is $\Theta(|G|)$
\item<3->$\Sigma' = \Sigma$
\item<4->If UNSAT$(G) = 0$ then UNSAT$(G') = 0$
\item<5->UNSAT$(G') \ge 2\min\{$UNSAT$(G), \alpha\}$
\end{itemize}
\end{block}

\begin{block}<6->{Step 1: Preprocessing}
Convert $G$ to a constant degree expander. Worsens UNSAT$(G)$ by a constant factor, blows up $|G|$ by a constant factor
\end{block}

\begin{block}<7->{Step 2: Power Step}
Assuming that the degree of $G$ is constant, we amplify UNSAT$(G)$ while blowing up $|G|$ and $|\Sigma|$ by a constant factor.
\end{block}
}

\frame{
\frametitle{The Gap Amplification Lemma}
\begin{block}<1->{Step 1A}
Convert $G$ in to a constant degree graph
\begin{itemize}
\item<2->Let $G_n$ be a family of expander graphs of degree $d$ and edge expansion at least $\varphi_0$
\item<3->$G$ is transformed as follows:
  \begin{itemize}
    \item<4->Each vertex $v\in V$ (with degree $d_v$) is replaced with expander graph $G_{d_v}$ and each edge incident on $v$ is assigned to a vertex in $G_{d_v}$.
    \item<5->All edges in $G_{d_v}$ have equality constraints. All other edges retain original constraints
    \item<6->Now, we have that $|V'| = \sum_{v\in V}d_v = 2|E|$ and $|E'|=frac{(d+1)|V'|}{2}=(d+1)|E|$, so $|G'|$ is $\Theta(|G|)$.
    \item<7->If UNSAT$(G)=0$, then $UNSAT(G')=0$ by assigning each vertex in $G_{d_v}$ to the color of $v$
    \item<8->Now, we need to show that if UNSAT$(G)\neq 0$, then UNSAT$(G')$ is not much smaller
  \end{itemize}
\end{itemize}
\end{block}
}

\frame{
\frametitle{The Gap Amplification Lemma}
\begin{block}<1->{Step 1A}
If UNSAT$(G)\neq 0$, then UNSAT$(G')$ is not much smaller
\begin{itemize}
\item<2->Let $\sigma': V'\mapsto \Sigma$ be the best coloring for $G'$
\item<3->We now obtain $\sigma : V\mapsto \Sigma$ where $\sigma(v)$ is the most popular color in $\{\sigma'(u)\mid u\in G_{d_v}\}$
\item<4->Let $\mu = $UNSAT$(G)$
\item<5->$B$ is the set of edges violated by $\sigma$ and $B'$ is the set of edges violated by $\sigma'$
\item<6->$S=\{v\in V'\mid \sigma'(v)$ is not the popular color$\}$
\end{itemize}
\end{block}
}

\frame{
\frametitle{The Gap Amplification Lemma}
\begin{block}<1->{Case 1: $|B'|\ge \frac{\mu|E|}{2}$}
UNSAT$(G') = \frac{|B'|}{|E'|}\ge\frac{\mu|E|}{2|E'|} = \frac{\mu}{2(d+1)} = \frac{\text{UNSAT}(G)}{2(d+1)}$
\end{block}

\begin{block}<2->{Case 2: $|S|\ge \frac{\mu|E|}{2}$}
\begin{itemize}
\item<3->Consider $v\in V$ and corresponding cloud $G_{d_v}$ in $G'$
\item<4->$S^v = $ vertices in $G_{d_v}$ that did not get the popular color
\item<5->$\forall a\in\Sigma$, $\Sigma_a^v=\{v\in\Sigma^v\mid \sigma'(v)=a\}$
\item<6->$|S_a^v|<\frac{d_v}{2}$
\item<7->From the expansion property, $|E(S_a^v, \bar{S_a^v})| \ge \varphi_0|S_a^v|$
\item<8->All edge constraints in $E(S_a^v, \bar{S_a^v})$ are violated!
\item<9->$|B'|\ge\frac{\sum|E(S_a^v,\bar{S_a^v})|}{2}\ge\frac{\varphi_0|S|}{2}\ge\frac{\mu\varphi_0}{4}|E|\ge\frac{\mu\varphi_0}{4(d+1)}|E'| = $UNSAT$(G)\frac{\varphi_0}{4(d=1)}$
\end{itemize}
\end{block}
{In both cases, since UNSAT$(G')$ is optimal, we have proven that UNSAT$(G) \le k\cdot$UNSAT$(G')$ for some constant $k$}
}

\frame{
\frametitle{The Gap Amplification Lemma}
\begin{block}<1->{Step 1B}
\begin{itemize}
\item<2->$G'$ is $(d+1)-regular$
\item<3->Superimpose $\tilde{d}$-regular expander on $|V'|$ nodes
\item<4->The new superimposed graph has the same vertex set as the original constraint graph, but its edges are the union of the 2 graphs ($G'$ and the expander)
\item<5->Add self-loops to each vertex to get $G''$
\item<6->Impose dummy constraints on each new edge
\item<7->$G''$ is still an expander with constant degree $d+2+\tilde{d}$
\item<8->
\end{itemize}
\end{block}
}

\frame{
\frametitle{Main Lemma}
\begin{block}{Main Lemma}
There exists as constant $\beta >0$ such that if UNSAT$(G)\le\frac1t$ then UNSAT$(G')\ge \beta\sqrt tUNSAT(G)$
\end{block}
\begin{block}{Proof:}
\begin{itemize}
\item Let $\sigma':V\mapsto \Sigma'$ be the best coloring for $G'$, hence $\alpha'=$UNSAT$(G')$ is the fraction of walk constraints violated by $\sigma'$
\item Let $\sigma:V\mapsto\Sigma$ be $\sigma(v) = \arg\max_{a\in\Sigma}\Pr[X_{v, t/2}=a]$ where $X_{v,i}$ is the opinion that a vertex has of $v$ that is $i$ random steps away from $v$
\item If $\sigma(v)=a$ then $\Pr[X_{v, t/2}=a] \ge \frac1\Sigma$
\item Let $B$ be the set of edges that are violated by $\sigma$ in $G$, then $\frac{|B|}{|E|} \ge$UNSAT$(G) = \alpha$
\end{itemize}
\end{block}
}

\frame{
\frametitle{Proof of Main Lemma}
We want to show that bad walks do not overlap very much
\begin{itemize}
  \item Let $w$ be a random $t$-walk in $G'$, we now define the following random variable:
    $$N = \begin{cases} \text{number of bad edges in }I & C_w\text{ is violated by }\sigma'\\ 0 & \text{otherwise}\end{cases}$$
  \item UNSAT$(G')\ge\Pr[N>0]$, we wish to lower bound $\Pr[N>0]$
  \item \underline{Claim 1:} $\exists\mu>0$ such that $E[N]\ge\frac{2\mu\sqrt t|B|}{|E|}$
  \item \underline{Claim 2:} $\exists C>0$ such that $E[N^2] \le \frac{C\sqrt t|B|}{|E|}$
  \item Chosing $\beta=\frac{4\mu^2}C$ completes the proof:
\end{itemize}
\begin{align*}
  \Pr[N>0] \ge& \frac{E[N]^2}{E[N^2]} \text{ (By the second moments inequality)}\\
  =&\; \frac{\frac{4\mu^2t|B|^2}{|E|^2}}{\frac{C\sqrt t|B|}{|E|}}
  = \frac{4\mu^2\sqrt t}C\frac{|B|}{|E|}
  \ge \beta\text{UNSAT}(G)\sqrt t
\end{align*}
}

\frame{
\frametitle{Proof of Claim 1}
\begin{itemize}
\item For a random walk $w$, define 2 random variables:
\begin{align*}
Z_i =&\; \begin{cases}
1, & i^\text{th}\text{ edge of }w\text{ is in }B\\
0, & \text{otherwise}
\end{cases}\\
Y_i =&\; \begin{cases}
1, & w\text{ is a rejecting }t\text{-walk and }Z_i=1\\
0, & \text{otherwise}
\end{cases}
\end{align*}
\item Note: $\forall i, Y_i\le Z_i$
\end{itemize}
}

\frame{
\frametitle{Proof of Claim 1}
\begin{itemize}
\item Let $N = \sum_{i\in I}Y_i$, then we have:
\begin{align*}
E[N] =&\; \sum_{i\in I}E[Y_i]\\
=&\; \sum_{i\in I}\Pr[Y_i=1]\\
=&\; \sum_{i\in I}\Pr[Y_i=1\mid Z_i=1]\Pr[Z_i=1]
\intertext{We know that $\Pr[Z_i=1] =\frac{|B|}{|E|}$ and $\Pr[Y_i=1\mid Z_i=1] \ge\frac{\tau^2}{|\Sigma|^2} = \mu$, so we get:}
\ge&\; \sum_{i\in I}\mu\frac{|B|}{|E|}\\
=&\; \frac{2\mu\sqrt t|B|}{|E|}
\end{align*}
\end{itemize}
}

\frame{
\frametitle{Proof of Claim 2}
\begin{itemize}
\item Since $Y_i\le Z_i$, we know that $N\le\sum_{i\in I}Z_i$, therefore:
\begin{align*}
  E[N^2] \le&\; E[(\sum_{i\in I}Z_i)^2]\\
    \le&\; 2\sum_{i\in I}\sum_{j\ge i}E[Z_iZ_j]\\
    =&\; 2\sum_{i\in I}\Pr[Z_i=1]\sum_{j\ge i}\Pr[Z_j=1\mid Z_i=1]\\
    =&\; 2\frac{|B|}{|E|}\sum_{i\in I}\sum_{j\ge i}\underbrace{\Pr[Z_j=1\mid Z_i=1]}_{\downarrow}
    \intertext{Probability that a random walk has its $(j-i+1)^\text{th}$ edge in $B$ given that the first edge in the walk is in $B$}
\end{align*}
\end{itemize}
}

\frame{
\frametitle{Proof of Claim 2}
\begin{align*}
  E[N^2] \le&\; 2\frac{|B|}{|E|}\sum_{i\in I}\sum_{j\ge i}\Pr[Z_j=1\mid Z_i=1]\\
  \le&\; 2\frac{|B|}{|E|}\sum_{i\in I}\sum_{j\ge i}\left( \frac{|B|}{|E|} + \lambda^{j-i} \right)\\
  \le&\; C\frac{|B|}{|E|}2\sqrt t\sqrt t\left(\frac1{\sqrt t}\right)\\
  \le&\; \frac{C\sqrt t|B|}{|E|}
\end{align*}
}


\end{document}
